---
title: "Derived Attributes and Dimensionality Reduction"
author: "By Prof T. Evgeniou edited heavily in this version by Dr. Cassy Dorff"
output:
  html_document:
    css: ../../AnalyticsStyles/default.css
    theme: paper
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    includes:
      in_header: ../../AnalyticsStyles/default.sty
editor_options: 
  chunk_output_type: console
---

# Get set up working on your machine

<!-- **Note:** Assuming the working directory is "MYDIRECTORY/INSEADAnalytics" (where you have cloned the course material), you can create an html file by running in your console the command rmarkdown::render("CourseSessions/Sessions23/FactorAnalysisReading.Rmd") -->

```{r setuplibraries, echo=FALSE, message=FALSE}
source("../../AnalyticsLibraries/library.R")
source("../../AnalyticsLibraries/heatmapOutput.R")
#devtools::install_github('cttobin/ggthemr')
#ggthemr('fresh')  # ggplot theme
theme_set(theme_bw())
opts_knit$set(progress=FALSE, verbose=FALSE)
opts_chunk$set(echo=FALSE, fig.align="center", fig.width=10, fig.height=6.5)
options(knitr.kable.NA = '')
```

```{r setup, echo=FALSE, message=FALSE}
# Load the data

# Please ENTER the name of the file with the data used. The file should be a .csv with one row per observation (e.g. person) and one column per attribute. 

datafile_name="MBAadmin" #  do not add .csv at the end, make sure the data are numeric, check your file.

# Please enter the minimum number below which you would like not to print - this makes the readability of the tables easier. Default values are either 10e6 (to print everything) or 0.5. Try both to see the difference.
MIN_VALUE=0.5

# Please enter the maximum number of observations to show in the report and slides 
# (DEFAULT is 15. If the number is large the report and slides may not be generated - very slow or will crash!!)
max_data_report = 10
ProjectData <- read.csv(paste0("data/", datafile_name, ".csv")) # this contains only the matrix ProjectData
ProjectData=data.matrix(ProjectData) 

if (datafile_name == "Boats")
  colnames(ProjectData)<-gsub("\\."," ",colnames(ProjectData))

# SET UP OF ALL THE INPUTS FOR THIS READING

# Please ENTER then original raw attributes to use. 
# Please use numbers, not column names! e.g. c(1:5, 7, 8) uses columns 1,2,3,4,5,7,8
factor_attributes_used= c(1:7)

# Please ENTER the selection criterions for the factors to use. 
# Choices: "eigenvalue", "variance", "manual"
factor_selectionciterion = "eigenvalue"

# Please ENTER the desired minumum variance explained 
# (ONLY USED in case "variance" is the factor selection criterion used). 
minimum_variance_explained = 65  # between 1 and 100

# Please ENTER the number of factors to use 
# (ONLY USED in case "manual" is the factor selection criterion used).
manual_numb_factors_used = 2

# Please ENTER the rotation eventually used (e.g. "none", "varimax", "quatimax", "promax", "oblimin", "simplimax", and "cluster" - see help(principal)). Defauls is "varimax"
rotation_used="varimax"

factor_attributes_used = unique(sapply(factor_attributes_used,function(i) min(ncol(ProjectData), max(i,1))))
ProjectDataFactor=ProjectData[,factor_attributes_used]
ProjectDataFactor <- data.matrix(ProjectDataFactor)
```

# Now we can get going with PCA

Why dimensionality reduction? One of the key steps in Data Analytics is to generate meaningful attributes starting from possibly a large number of **raw attributes**, variables, or features.

In cases where we have a lot of data and a clear question (such as, are there subgroups of customers that help us understand purchasing patterns?), it takes creative effort and time - based on a deep contextual knowledge - to generate new  attributes (e.g. "this customer is price sensitive", or "this customer likes comfort", or "this customer is status conscious", etc) from the original raw ones, which we call here **derived attributes** or **new features**. These attributes can be generated manually, or using what we call **data or dimensionality reduction techniques**. 

> Before proceeding on understanding the statistical techniques considered here, it is important to note that this is not the only approach to generating meaningful derived attributes from large numbers of raw ones: there is always "the manual approach" based on contextual knowledge and intuition, which can probably take a data analytics project already very far. However, in terms of mathematical techniques used in data analytics, factor analysis is one of the key ones when it comes to generating new meaningful derived attributes from the original raw ones.

## The "Business Decision"

We consider the core decision of an MBA admissions committee: *which applicants should we accept in the MBA program?* The school is interested in predicting the MBA participant's success in the future before offering admission (probably some ethical issues with this example).

## The Data

To make this decision, the committee uses a number of data about the applicants. Let us consider for example the following attributes in evaluating an MBA application (of course in practice many more can be considered):

1. GPA 
2. GMAT score 
3. Scholarships, fellowships won 
4. Evidence of Communications skills (debating competition, personal interview score) 
5. Prior Job Experience 
6. Organizational Experience 
7. Other extra curricular achievements 

Let us assume that this data is converted into a numerical scale from 1-7. For example: a numerical rating may be given to the fellowships based on the prestige and the number of fellowships won. Job experience may be rated on the number of years on the job, with a numerical weighting for the level of the job in the managerial ladder. For simplicity's sake in this example, we just need to understand they are all measured low to high from 1 to 7.

This is how the first `r min(max_data_report, nrow(ProjectData))` data looks:

```{r}
rownames(ProjectDataFactor) <- paste0("observation ", sprintf("%02i", 1:nrow(ProjectDataFactor)))
round(ProjectDataFactor, 2)[1:min(max_data_report,nrow(ProjectDataFactor)), ]
```

## The Approach

How can this data inform the school's admission decisions? Intuitively it may seem that the data above capture two fundamental abilities that affect the success of students in their management careers: 

1. Education 
2. Team and Leadership skills. 

The school may be interested for example in picking students who score high on these two areas. In this case, of course the admissions committee in theory could just ask the applicants two questions: 

1. "How well did you do in school?" 
2. "How strong are your team and leadership skills?" 

As you can imagine, asking these questions would not only make the admissions interviewers look naive, but would also lead to very noisy and misleading answers: of course everyone will just answer both questions with the highest mark!

So instead of asking these "naive" questions, the school is using *raw attributes/data* like the ones above, which can also be gathered more easily The idea then is to see how this data can be "translated" in meaningful derived attributes that, for example, could capture the "equivalent answers" one could get if one were to ask directly the two naive questions above - or possibly other such "complex" questions. 

> Factor analysis, generally, is a statistical approach for finding  a few "hidden" derived attributes in data by combining together groups of the original raw attributes in such a way that the least information in the original data is lost - in a statistical sense. It is part of a general class of statistical methodologies used to do what is often called "dimensionality reduction". PCA is a type of dimensionality reduction. </p> </blockquote>

Back to our example, if there is some way in which we could reduce the `r ncol(ProjectData)` attributes into a smaller set of, say, 2 or 3 attributes, then we can reduce the data to a more understandable form so that our decision making process can be made potentially simpler, more actionable, and easier to interpret and justify - without losing much information in the original data. It is  much easier to make tradeoffs between two or three attributes than it is between 10 or 20 attributes (look at any survey or application form and you will see that there are easily more than 20 questions). Hence, 

> Data reduction is a very useful step in helping us make decisions, particularly for prediction and modeling.

# A 6-steps Process for Dimensionality Reduction

> It is important to remember that Data Analytics Projects require a delicate balance between experimentation, intuition, but also following (once a while) a process to avoid getting fooled by randomness and "finding results and patterns" that are mainly driven by our own biases and not by the facts/data themselves.

There is *not one* process for PCA. However, we have to start somewhere, so we will use the following process:

1. Confirm the data is metric 
2. Decide whether to scale or standardize the data
3. Check the correlation matrix to see if PCA makes sense
4. Develop a scree plot and decide on the number of components to be derived
5. Interpret the components, a little
6. Save component scores for subsequent analyses

Let's follow these steps.

## Step 1: Confirm data is metric

Steps 1-3 are about specific descriptive characteristics of the data. In particular, the methods we consider in this note require that the data are *metric* (step 1): this means not only that all data are numbers, but also that the numbers have an actual numerical meaning, that is 1 is less than 2 which is less than 3 etc. 

If we have other types of data (e.g. gender, categories that are not comparable, etc), there are other methods to use. However, for now we will only consider a specific method, which we will also mis-use for non-numeric data for simplicity. 

The data we use here have the following descriptive statistics: 

```{r}
round(my_summary(ProjectDataFactor), 2)
```
> Note that one should spend a lot of time getting a feeling of the data based on  summary statistics and visualizations and EDA!: good data analytics require that we understand our data very well.

## Step 2: Scale the  data

Note that for this data, while 6 of the "survey" data are on a similar scale, namely 1-7, there is one variable that is about 2 orders of magnitude larger: the GMAT variable. Having some variables with a very different range/scale can often create problems: **most of the "results" may be driven by a few large values**, more so than we would like. 

To avoid such issues, one has to consider whether or not to **standardize the data** by making some of the initial raw attributes have, for example,  mean  0 and standard deviation 1 (e.g. `scaledGMAT = (GMAT-mean(GMAT)) / sd(GMAT)`), or scaling them between 0 and 1 (e.g. `scaledGMAT = (GMAT-min(GMAT)) / (max(GMAT)-min(GMAT))`). Here is for example the R code for the first approach, which is a very common approach:

**CLASS NOTE** Apply functions: You have learned how to use map functions in `purrr`, such map functions are more regularized in naming. Apply functions are similar. Check ?apply to learn more. Good idea to know what these are and how they work, as a data scientist. 

```{r, echo=TRUE, tidy=TRUE}
ProjectDatafactor_scaled=apply(ProjectDataFactor,2, function(r) {if (sd(r)!=0) res=(r-mean(r))/sd(r) else res=0*r; res})
```

Notice now the summary statistics of the scaled dataset:

```{r}
round(my_summary(ProjectDatafactor_scaled), 2)
```

As expected all variables have mean 0 and standard deviation 1. 

While this is typically a necessary step, one has to always do it with care: some times you may want your analytics findings to be driven mainly by a few attributes that take large values; other times having attributes with different scales may imply something about those attributes. For example, when students rate their schools on various factors on a 1-7 scale, if the variability is minimal on a certain variable (e.g. satisfaction about the IT infrastructure of the school) but very high on another one (e.g. satisfaction with job placement), then standardization will reduce the real big differences in placement satisfaction and magnify the small differences in IT infrastructure satisfaction. In many such cases one may choose to skip step 2 for some of the raw attributes. Hence, in some cases, standardization is not a necessary data transformation step, and you should use it judiciously. 

## Step 3:  Check correlations 

The type of dimensionality reduction methods we will use here "group together raw attributes that are highly correlated". Other methods (there are many!) use different criteria to create derived variables. 

For this to be feasible, it is necessary that the original raw attributes do have large enough correlations (e.g. more than 0.5 in absolute value, or simply statistically significant). It is therefore useful to see the correlation matrix of the original attributes - something that one should anyway always do in order to develop a better understanding of the data. 

```{r}
thecor = round(cor(ProjectDataFactor),2)
colnames(thecor)<-colnames(ProjectDataFactor)
rownames(thecor)<-colnames(ProjectDataFactor)
round(thecor,2)
```

There are quite a few large (in absolute value) correlations. For example  GPA, GMAT and Fellowship seem to be highly positively correlated - as expected? 

Maybe those can be grouped in one "factor" or "component"? How about "Communication Skills"? Should that also be part of that same factor? With what weights should we combine these raw attributes in groups? Remember, this is a very simple example where one could possibly derive attributes manually. It is good to spend time thinking through it.

Let's now see what PCA suggests as factors (components).

## Step 4: Choose number of components 

There are many statistical methods to generate derived variables from raw data. One of the most standard ones is **Principal Component Analysis**. 

This method finds factors, called **Principal Components**, which are **linear combinations of the original raw attributes** so that most of the information in the data, measured using  **variance explained** (roughly "how much of the variability in the data is captured by the selected components") is captured by only a few factors. 

The components are developed typically so that they are **uncorrelated**, leading to *at most as many components as the number of the original raw attributes, but* so that only a few are needed (the *principal components*) to keep most of the information (variance/variability) in the raw data. For example, for our data we have `r ncol(ProjectData)` raw attributes hence we can only have a total of `r ncol(ProjectData)` factors/components, each of them being a linear combination of the `r ncol(ProjectData)` original raw data. 

```{r}
# principal() is from the psych package
# does eigen value decomposition + PCA if you set score=TRUE
# we are just using this to look at eigenvalues
# original code from tutorial author:
UnRotated_Results<-principal(ProjectDataFactor, nfactors=ncol(ProjectDataFactor), rotate="none",score=TRUE)

# round the results from pca function above
# and grabs the loadings information, in this package, this will give you the coordinates
# for variables in your data
# this will help you "see" where the components are loading onto your variables in your data!
UnRotated_Factors<-round(UnRotated_Results$loadings,2)

# put into a dataframe
UnRotated_Factors<-as.data.frame(unclass(UnRotated_Factors))
colnames(UnRotated_Factors)<-paste("Component",1:ncol(UnRotated_Factors),sep=" ")
print(UnRotated_Factors)

# dorff code:
library(factoextra)

# rename data
mba_data = ProjectDataFactor

# conduct PCA analysis with prcomp function
# creats PCA output object


# now that we have checked that
# we can dive into the PCA results a bit more
# this "get_pca_var" gives us the PCA information for the variables in our data


# look at PCA coordinates 
# this is the same as print(UnRotated_Factors) above!

```

By looking at these coordinates we can 'see' where the data is 'loading' on the PCA projection. (Recall: PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate).

While there are as many (and for other methods, more) factors as the number of the original raw attributes, since our goal is to have a small(er) number of derived variables/factors, one question is whether we could use only a few of the components without losing much information. 

When this is feasible, we can say that the original raw attributes can be "compressed" or "reduced" to a few principal components/factors/derived variables and that, in this case, PCA performed well. 

When using PCA, we have two measures of "how much of the information (variance in this case) in the original raw data is captured by any of the factors/components": 

a) the *percentage of variance explained* by components
b) the *eigenvalue coresponding to the component*

Each component has an eigenvalue as well as the percentage of the variance explained. The sum of the eigenvalues of the components is equal to the number of original raw attributes used for PCA, while the sum of the percentages of the variance explained across all components is 100%. For example, for our data these are:

```{r}
# main calculation using a different PCA function than what author used above
Variance_Explained_Table_results<-PCA(ProjectDataFactor, graph=FALSE)

# the author is just cleaning up the table here
# focus on interpreting the table, because this is the kind of output
# you see in other packages
Variance_Explained_Table<-Variance_Explained_Table_results$eig
Variance_Explained_Table_copy<-Variance_Explained_Table
rownames(Variance_Explained_Table) <- paste("Component", 1:nrow(Variance_Explained_Table))
colnames(Variance_Explained_Table) <- c("Eigenvalue", "Pct of explained variance", "Cumulative pct of explained variance")

round(Variance_Explained_Table, 2)

# Dorff code
# again, more simple with prcomp() package, and then you only use one package
# for calculating
# compare eigenvalue differentiation across components
# eigen values using 'get_eigenvalue' function
# this also returns same info as author table above
# with variance percent and cumulative variance explained!


```

Review:

1. We have conducted PCA using a few different packages in R, but I recommend you use `prcomp()` from the `stats` package.

2. Then we looked at how well the PCA components mapped onto the variables in our data!

3. Then we checked out our eigenvalues, are they above 1? How much variance do they explain in the data? This helps us understand how well PCA is doing at capturing variance in our data.

- Note that the "first principal component" has the highest eigenvalue and captures most of the information (variance in this case) of the original raw data. 

4. Next we will look at this visually. 

> Two Statistical criteria to select the number of factors/derived variables when using PCA are: a) select components with corresponding eigenvalue larger than 1 (this is only sometimes true, but is based on the idea the average eigenvalue will be 1, so you want a higher than average value); b) Select the components with the highest eigenvalues "up to the component" for which the cumulative total variance explained is relatively large (e.g. more than 50%).

One can also plot the eigenvalues of the generated factors in decreasing order: this plot is called the **scree plot**. For our data this plot looks as follows:

```{r figure01}
# again, original author's codde
eigenvalues  <- Variance_Explained_Table[, "Eigenvalue"]
df<- cbind(as.data.frame(eigenvalues), c(1:length(eigenvalues)), rep(1, length(eigenvalues)))
colnames(df) <- c("eigenvalues", "components", "abline")
ggplot(melt(df, id="components"), aes(x=components, y=value, colour=variable)) + geom_line()

# dorff version 
# visualize and inspect results
# factoextra package


# recall, the output for variance explained will help you interpret this plot
print(eig.val)
```

Note, there are loads of packages that will give you elbow plots! 

> A third rule of thumb to decide how many components to use is to consider only the factors up to the "elbow" of the scree plot.

Based on the three criteria (eigenvalue > 1, cumulative percentage of variance explained, and the elbow of the scree plot), and using our current selection criterion, we can make an informed choice about how many 'components' help to capture variation in our data. We can then decide to use these components, rather than the original variables in future models like regression. In this case, you might choose 2 or 3. The elbow plot is at 3, but then eigenvalues might suggest 2. You could, later validate your choice via cross-validation and predictive performance if you take these components and stick them into a model. 

## Step 5: Interpret the factors 

How would you interpret the selected factors?

There are additional visualizations that can help you make intuitive sense of these findings. I recommend using the `fviz` suite of plots from the `factoextra` package.

```{r}
# factoextra
fviz_pca_var(pca_out,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#FAAE7B", "#432371"),
             repel = TRUE     # Avoid text overlapping
             )

# this would be more useful if the individuals had names, i.e. imagine these are cities
fviz_pca_biplot(pca_out, repel = TRUE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969"  # Individuals color
                )
```

## Step 6: Save scores 

Once we decided the factors to use (for now), we typically replace the original data with a new dataset where each observation (row) is now described not using the original raw attributes but using instead the selected factors/derived attributes (or has a combination of the two, like in the visualization shown below). After all this was the goal of this analysis. 

The way to represent our observations using the found derived attributes (factors/components) is to estimate for each observation (row) how it "scores" for each of the selected factors. These numbers are called **factor scores** or **PCA scores**. Effectively they are the "scores" the observation would take on the factor had we measured that factor directly instead of measuring the original raw attributes. 

**Note:** Sometimes for simplicity we represent each selected factor using one of the original raw attributes, typically the one on which the factor has the highest loading on. Although this is not statistically as accurate, it helps with the interpretation of subsequent analyses.

Can you describe the observations using the new derived variables? How does each person perform for each of the selected factors? These are questions you can continue to explore by now digging into how the components map onto the data.

As always remember that:

> Data Analytics is an iterative process, therefore we may need to return to our original raw data at any point and select new raw attributes as well as new factors and derived variables.

```{r}

# Contributions of variables to PC1
fviz_contrib(pca_out, choice = "var", axes = 1, color="GPA")
fviz_contrib(pca_out, choice = "var", axes = 2)

```